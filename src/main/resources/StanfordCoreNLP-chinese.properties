# 基本設置
annotators = tokenize, ssplit, pos
tokenize.language = zh

# 中文分詞設置
segment.model = edu/stanford/nlp/models/segmenter/chinese/ctb.gz
segment.sighanCorporaDict = edu/stanford/nlp/models/segmenter/chinese
segment.serDictionary = edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz
segment.sighanPostProcessing = true

# 詞性標注設置
pos.model = edu/stanford/nlp/models/pos-tagger/chinese-distsim.tagger

# 其他優化設置
ssplit.boundaryTokenRegex = [。]
ssplit.isOneSentence = true